{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluating Models with ROC Curves\n",
    "\n",
    "*  Receiving Operating Characteristic, or ROC, is a visual way for inspecting the performance of a binary classification algorithm. \n",
    "*  In particular, it's comparing the rate at which your classifier is making correct predictions (\\textit{True Positives} or TP) and the rate at which your classifier is making false alarms (\\textit{False Positives} or FP). \n",
    "*  When talking about True Positive Rate (TPR) or False Positive Rate (FPR) we're referring to the definitions below:\n",
    "\n",
    "\n",
    "$$ \\mbox{TPR}= \\frac{\\mbox{True Positives}}{\\mbox{True Positives + False Negatives} } $$\n",
    "$$ \\mbox{FPR}=\\frac{\\mbox{False Positives}}{\\mbox{False Positives + True Negatives} }$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Remark \n",
    "\n",
    "* True Positives Rates and True Negatives Rates referred to as **Sensitivity** and **Specificity**.\n",
    "\n",
    "* we're measuring the trade off between the rate at which you can correctly predict something, with the rate at which you make an embarrassing blunder and predict something that doesn't happen.\n",
    "\n",
    "----------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Background \n",
    "\n",
    "*  ROC curves were first used during WWII to analyze radar effectiveness. \n",
    "*  In the early days of radar, it was sometimes hard to tell a large bird from an incoming airplane. \n",
    "*  The British Ministry of Defence pioneered using ROC curves to optimize the way that they could rely to radar for detect approaching Luftwaffe airplanes.\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#### Scenarios: Guessing at Random \n",
    "\n",
    "*  The first example is the simplest: a diagonal line. \n",
    "*  A diagonal line indicates that the classifier is just making completely random guesses.\n",
    "*  Since your classifier is only going to be correct 50\\% of the time, it stands to reason that your TPR and FPR will also be equal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (4108355461.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_228/4108355461.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \\begin{figure}[h!]\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\\begin{figure}[h!]\n",
    "\\centering\n",
    "\\includegraphics[width=0.7\\linewidth]{images/roc-guessing}\n",
    "\\end{figure}\n",
    "\n",
    "Often, ROC charts will include the random ROC curve to provide the user with a benchmark for what a naive classifier would do.\\\\  Any curves above the line are better than guessing, while those below the line, you would be better off guessing.\n",
    "\n",
    "#### For review\n",
    "\n",
    "* The Area Under the Curve (AUC)  is 0.500.\n",
    "\n",
    "### A Perfect Classifier\n",
    "\n",
    "*  A perfect classifier will yield a perfect trade-off between TPR and FPR (meaning you'll have a TPR of 1 and an FPR of 0).\n",
    "*  In that case, your ROC curve looks something like this.\n",
    "\n",
    "\n",
    "\\begin{figure}[h!]\n",
    "\\centering\n",
    "\\includegraphics[width=0.7\\linewidth]{images/roc-perfect}\n",
    "\\end{figure}\n",
    "\n",
    " <b>Important:</b> The better your classifier, the more closer the curve will be to the top left corner.\n",
    "\n",
    " \\textit{For review: Note the \"random curve\" is included as a benchmark as a dotted line.\\\\ \n",
    "The Area Under the Curve (AUC) is 1.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (871778167.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_228/871778167.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \\textbf{Worse than guessing}\\\\\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    " \\textbf{Worse than guessing}\\\\\n",
    "\n",
    " A bad classifier (i.e. something that's worse than guessing) will appear mostly below the random line. \n",
    "\n",
    "\\begin{figure}[h!]\n",
    "\\centering\n",
    "\\includegraphics[width=0.9\\linewidth]{images/roc-bad}\n",
    "\\end{figure}\n",
    "\n",
    " There have been several instances of a ``prediction system\" underperforming \"guessing at random\".\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Better than guessing\n",
    "\n",
    "A much more interesting activity is attempting to decipher the difference between an \"OK\" and a \"Good\" classifier. The chart below shows an example of a very mediocre classifier. It is still better than guess at random though.\n",
    "\n",
    "\\begin{figure}[h!]\n",
    "\\centering\n",
    "\\includegraphics[width=0.9\\linewidth]{images/roc-ok}\n",
    "\\end{figure}\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "#### Reasonably Good \n",
    "\n",
    "%Ahh this is looking a little better. Below you can see a nice \"hump shaped\" (it's a technical term) curve that's continually increasing. It sort of looks like it's being yanked up into that top left (the perfect) spot of the chart.\n",
    "\n",
    "In practice, most decent classification systems have a ROC curve like this.  Recall that better a prediction system is , the closer it is to the top left.\n",
    "\\begin{figure}[h!]\n",
    "\\centering\n",
    "\\includegraphics[width=0.9\\linewidth]{images/roc-pretty-good}\n",
    "\n",
    "\\end{figure}\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "                                                                              \n",
    " \\textbf{Area under the curve (AUC)}\n",
    "\n",
    " There is an aggregate metric to determine how good the prediction system is:  AUC or Area Under the Curve. \n",
    "\n",
    " The AUC is the amount of space underneath the ROC curve\n",
    "\n",
    "\n",
    "*  AUC = 0 :  Perfectly Bad\n",
    "*  AUC $< 0.5$ : Worse than guessing at random \n",
    "*  AUC = 0.5 : same as guessing at random\n",
    "*  AUC $> 0.5$ : Good. better than guessing at random\n",
    "*  AUC = 1 : Perfectly Good\n",
    "\n",
    "%\n",
    "%\n",
    "%\n",
    " Comparing AUC values is useful when comparing different models, as we can select the model with the high AUC value, rather than just look at the curves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}